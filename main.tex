% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
%\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[dvipsnames]{xcolor}
\usepackage{bm}
\usepackage[makeroom]{cancel}
\usepackage[inline]{enumitem}
\usepackage{cleveref}

\usepackage[ruled,vlined]{algorithm2e}
%\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
%\SetCommentSty{mycommfont}
\SetKwComment{Comment}{$\triangleright$\ }{}

\usepackage[%
square,        % for square brackets
comma,         % use commas as separators
%  numbers,       % for numerical citations;
%  sort,          % orders multiple citations into the sequence in which they appear in the list of references;
%sort&compress, % as sort but in addition multiple numerical citations
% are compressed if possible (as 3-6, 15);
]{natbib}
% In the bibliography, references have to be formatted as 1., 2., ... not [1], [2], ...
\renewcommand{\bibnumfmt}[1]{#1.}

\renewcommand{\bibsection}{\section*{References}} % requried for natbib to have "References" printed and as section*, not chapter*
% Use natbib compatbile splncsnat style.
% It does provide all features of splncs03, but is developed in a clean way.
% Source: http://phaseportrait.blogspot.de/2011/02/natbib-compatible-bibtex-style-bst-file.html
\bibliographystyle{splncsnat}

\usepackage{todonotes}
\newcommand{\oam}[1]{\todo[inline,color=orange!40]{{\textbf{OM:}~}#1}}
\newcommand{\el}[1]{\todo[inline,color=green!40]{{\textbf{EL:}~}#1}}

\input{mathdef.tex}
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
 \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}


\title{Graph-Based Optimistic Planning}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Edouard Leurent\inst{1,2}\and Odalric-Ambrym Maillard\inst{1}}
%
\authorrunning{E. Leurent and O-A. Maillard}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{SequeL project, INRIA Lille - Nord Europe, France\\
\email{$\{$edouard.leurent,odalric.maillard$\}$@inria.fr}\and
Renault Group, France
%\\
%\email{edouard.leurent@renault.com}
}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
	We consider the problem of planning in a Markov Decision Process (MDP) with a generative model and limited computational budget. Despite the underlying MDP transitions having a graph structure, the popular \emph{tree-based} planning algorithms such as \texttt{UCT} rely on a tree structure to represent their value estimates. That is, they either ignore the states crossed along a trajectory, or they do not identify with each other two similar states met at different places in the tree. In this work, we explore taking into account state similarity by proposing a \emph{graph-based} planning algorithm. For the sake of analysis and explicit comparison, we first study our algorithm by framing it as a tree-based planner augmented with a \emph{state merge} operator, in the simple case of deterministic systems. We provide a regret bound that depends on a novel problem-dependent measure of difficulty. This measure improves on the previous one in MDPs where the trajectories overlap, and recovers it otherwise. Then, we show that this methodology can be adapted to existing planning algorithms that deal with stochastic systems. Finally, numerical simulations illustrate the benefits of our approach.
	
	\keywords{Planning  \and Tree-search \and Reinforcement Learning.}
\end{abstract}
%
%
%

\section{Introduction}

In a \emph{Markov Decision Process} (MDP), an agent observes its current state $s$ from a state space $S$ and picks an action $a$ from an action space $A$ of size $K$, before transitioning to a next state $s'$ drawn from a transition kernel $\probability{s'\condbar s,a}$ and receiving a bounded reward $r\in[0, 1]$ drawn from a reward kernel $\probability{r \condbar s, a}$. The transition kernel can be seen as a \emph{graph}, where the nodes correspond to states $s\in S$, and the edges to transition probabilities between two states. The goal of the agent is act so as to maximise in expectation its cumulative discounted rewards $\sum_{t=0}^\infty \gamma^t r(s_t, a_t)$, where $\gamma\in(0, 1)$ is a discount factor. This amounts to choosing at each step the action that maximises the state-action value function $Q$ defined as:
\begin{align*}
Q(s, a) &= \max_\pi  \expectedvalue_\tau \left[\sum_{t=0}^\infty \gamma^t r(s_t, a_t)\condbar s_0 = s, a_0 = a\right]
\end{align*}
where $\pi$ is a policy mapping states to actions and $\tau = (s_0, a_0, \dots)$ is a trajectory generated by following $\pi$. In an \emph{online planning} setting, the underlying MDP is \emph{unknown} to the agent which only has access to a \emph{generative model} that provides samples $s'$ of $\probability{s' \condbar s, a}$ when queried. Then, under computational constraints known as \emph{fixed-budget}, the agent is allowed to generate a limited number $n$ of trajectories starting from its current state before recommending a good action $a_n$ to take next.
The quality of that action is assessed in terms of the \emph{simple regret}
\begin{align}
	r_n \eqdef V(s) - Q(s, {a}_n), \; \mbox{where} \; V(s) \eqdef  \max_a Q(s, a).
\end{align}

Tree-based planning algorithm date back to the work of \citet{Kearns02SS}, who proposed a uniform sampling approach for planning in MDPs, called \texttt{Sparse} \texttt{Sampling}. This uniform exploration strategy was further analysed with the \texttt{BRUE} algorithm
\citep{Feldman14BRUE} providing an enhanced value estimation procedure. Another family of algorithms rely on the principle of \emph{Optimism in the Face of Uncertainty} \citep[surveyed by][]{SurveyRemiMCTS}, inspired from the Multi-Armed Bandit problem. This principle was first used in the context of planning in the \texttt{CrazyStone} software \citep{Coulom2006} for computer Go. It was later analysed as the \texttt{UCT} algorithm \citep{Kocsis06UCT}, but was shown by \citet{Coquelin2007} to have a doubly-exponential complexity in the worst case. The {Optimistic Planning for Deterministic Systems} (\texttt{OPD}) algorithm introduced by \citet{hren2008optimistic} was the first to provide a polynomial regret bound, but was limited to sytems with deterministic rewards and dynamics. It was then extended to the {open-loop} setting to handle stochastic rewards \citep{bubeck2010open,leurent2019practical}.
MDP with known parameters \citep{busoniu2012optimistic}.
For MDPs with stochastic and unknown transitions, polynomial sample complexities have been obtained for StOP \citep{STOP14}, TrailBlazer \citep{TrailBlazer16} and SmoothCruiser \citep{SmoothCruiser19} but the three algorithms suffer from numerical inefficiency. Indeed, StOP requires to explicitly reason about policies and storing them is very costly, while TrailBlazer and SmoothCruiser require a very large amount of recursive calls even for small MDPs.



\emph{All} the planning algorithms mentioned so far have one thing in common: they \emph{do not merge information across states}.
% A node is identified with the sequence of $h$ states and actions that leads to it. It leads to a very simple update: after each trajectory, one only needs to update the confidence bounds, $U_h(s_h,a_h)$ and $L_h(s_h,a_h)$, of the visited action-state pairs, see Algorithm~\ref{alg:OptimisticVItraj}. %The complexity of this value iteration is only of order $O(KBH)$ in time and $O(H)$ in space (since we only need to keep the last trajectory).
% Another option is to merge information for the same states and a fixed depth. But in this case the search tree becomes a \emph{graph} and after each trajectory we need to re-compute the values $U_h(s_h,a_h)$ for all stored state action pairs $(s_h,a_h)$ at each depth, see
% %we thus need to update  perform a complete optimistic value iteration to update the confidence bounds, see
% Algorithm~\ref{alg:OptimisticVIcompl}.
% %The complexity of this algorithm is of order $O(KBH)$ in time and $O(H)$.


If two trajectories $\tau_1$, $\tau_2$ lead to the same state $s$, the latter is actually modelled as two different states $s^1$ and $s^2$ and the information is not shared between the two trajectories. A local update can require to propagate information through the entire tree. For instance, in \Cref{fig:structures} (left), two different paths lead to a state in red. The information gathered present in the subtree of the left branch is not used to update the estimates at the leaf.

\paragraph{Contributions}

\paragraph{Outline}


\section{Graph-Based Planning for Deterministic Systems}

We consider the simple setting of MDPs with deterministic dynamics and rewards.
We start by giving some background on the interplay of data structures and optimistic planning algorithms.

\subsection{Data structures}

In this work, we will be comparing two data structures for planning in an MDP: the tree and the (directed) graph, represented in \Cref{fig:structures}. In order to differentiate them, we use Roman symbols when referring to the trees, e.g. $T, U, L, B$ and calligraphic symbols when referring to the graphs, e.g. $\cG, \cU, \cL, \cB$.

\begin{figure}[tp]
	\centering
	\includegraphics[trim={1.8cm 1.2cm 1.9cm 1.2cm}, clip,width=0.44\linewidth]{img/tree_1}
	\hfill
	\includegraphics[trim={1.8cm 1.2cm 1.9cm 0.9cm}, clip,width=0.44\linewidth]{img/graph_1}
	\caption{Depiction of the tree $\Tau_n$ (left) and the graph $\cG_n$ (right) built with the same observed transitions. In the tree, two nodes of the same colour lead to the same state.}
	\label{fig:structures}
\end{figure}

\paragraph{In a tree,} a node of depth $h$ represents a sequence of actions $a\in A^h$. The \textit{root} of the tree corresponds to the empty action sequence, and hence to the initial state $s_0\in S$. At iteration $n$, we denote the current tree as $\Tau_n$. Borrowing notations from topology, we denote its set of internal nodes as $\inte{\Tau}_n$ and its set of exterior nodes (the leaves) as $\ext{\Tau}_n$. Note that since the MDP is deterministic, a sequence of action $a$ is associated with its terminal state denoted $s(a)$, but this association is not one-to-one: several sequences of action can lead to the same state, and the latter will be represented several times in the tree. Thus, we denote $N_n(a)$ the set of \emph{neighbours} of a sequence of action $a\in A^h$, that lead to the same state:  $N_n(a) = \{a'\in\Tau_n: s(a)=s(a')\}.$

\paragraph{In a graph,} the nodes represent states $s\in S$, and the edges represent transitions between states. The \textit{source} of the tree corresponds to the initial state $s_0$. At iteration $n$, we denote the current graph as $\cG_n$, its set of internal nodes as $\inte{\cG}_n$ and its set of \textit{sinks} as $\ext{\cG}_n$.

Both structures are built iteratively from a single starting node, by selecting an exterior node (leaf or sink) to \emph{expand}. The expansion of a node $a$ or $s$ refers to calling the generative model to sample the reward $r$ and next state $s'$ for each action $a\in A$, and adding child nodes to the data structure. In a tree, the expansion of a node $a\in A^h$ always lead to the creation of new leaf nodes that represent the suffix sequence of action $ab\in A^{h+1},\, b\in A$. In contrast, in a graph the next state $s'$ reached from $s,a$ might already be present in $\cG_n$, in which case we add the edge between $s$ and $s'$ without creating a new node.

These data structures can be used to store information about the MDP, such as the transitions and rewards $r(s, a)$, or other informations useful for planning.


\subsection{Optimistic planning}

A planning algorithm is typically composed of two main rules:
\begin{enumerate}
	\item A \emph{sampling rule}, that selects a promising exterior node (leaf or sink) to expand at each iteration $n$.
	\item A \emph{recommendation rule}, that recommends a first good action to take at the last iteration $n$.
\end{enumerate}
The pseudo-code of generic planning algorithm is given in \Cref{alg:generic}.
\begin{algorithm}
	\caption{A generic planning algorithm}
	\label{alg:generic}
	\DontPrintSemicolon
	\For{each iteration $n$}{
		Select the node $\hat{a}_n$ (or $\hat{s_n}$) to expand according to the \textbf{sampling rule}.\;
		\For(\Comment*[f]{Node expansion}){action $a\in A$}{
			Simulate the transition $r, s' \sim \probability{r, s' \condbar s, a}$ from the generative model.\;
			Insert the observed transition to the data structure accordingly.
		}
	}
	\Return the recommended action $a_{n}$ according to the \textbf{recommendation rule}.\;
\end{algorithm}

These sampling and recommendation rules must be chosen with the goal of minimising the simple regret $r_n$.
A very popular approach is to follow the principle of \emph{Optimism in the Face of Uncertainty} (OFU) \citep{SurveyRemiMCTS}, which consists in exploring the option that maximises an upper-bound of the true objective. In the context of planning, it can be applied by forming bounds on the state value function $V$.

\begin{definition}[Value bounds]
\paragraph{\textbf{On trees.}} We denote by $L:\Tau_n \rightarrow \Real$ and  $U:\Tau_n \rightarrow \Real$ a lower-bound and upper-bound for the state value $V$ defined on the tree $\Tau_n$, such that
\begin{equation*}
    \forall a\in\Tau_n, \qquad L(a) \leq V(s(a)) \leq U(a).
\end{equation*}

\paragraph{\textbf{On graphs.}} Likewise, we denote by $\cL:\cG_n \rightarrow \Real$ and  $\cU:\cG_n \rightarrow \Real$ a lower-bound and upper-bound for the state value $V$ defined on the graph $\cG_n$, such that
\begin{equation*}
\forall s\in\cG_n, \qquad \cL(s) \leq V(s) \leq \cU(s).
\end{equation*}
\end{definition}

Following the OFU principle, at iteration $n$ we must leverage available information to design an upper-bound $U_n$ (or $\cU_n$) on $V$ as tight as possible. Then, in order to select an promising exterior node to expand, the sampling rule starts from the root (or source) and follows the optimistic strategy of always selecting the action which maximises $U_n$ (or $\cU_n$), until reaching an optimistic leaf (or sink) to expand. This strategy was used in \citep[e.g.][]{Kocsis06UCT, hren2008optimistic, bubeck2010open, busoniu2012optimistic}.

For instance, since we assume that the rewards are bounded in [0, 1], trivial bounds on $V(s)$ are
$0 \leq V(s) \leq V_{\max} \eqdef {1}/({1-\gamma})$. But these trivial bounds are the same for every node, which makes them non-informative, and do not make use of the observed information. However, they can be used as a valid starting point. Every observed transition stored in the graph / can then be used to tightened these bounds, by using the Bellman Optimal operator.

\begin{definition}[Bellman Optimal operator]
	\paragraph{\textbf{On trees.}} We define the Bellman Optimal operator $B_n$ on the tree $\Tau_n$ as:
	\begin{equation}
	\label{eq:bellman-tree}
	B_n(f)(a) = \begin{cases}
	f(a) & \text{if $a\in\ext{\Tau}_n$;} \\
	\max_{b\in A} r(ab) + \gamma f({ab})
	& \text{if $a\in\inte{\Tau}_n$.}
	\end{cases}
	\end{equation}
	
	\paragraph{\textbf{On graphs.}} Likewise, we define the Bellman Optimal operator $\cB_n$ on the graph $\cG_n$ as:
	\begin{equation}
	\label{eq:bellman-graph}
	\cB_n(f)(s) = \begin{cases}
	f(s) & \text{if $s\in\ext{\cG}_n$;} \\
	\max_{b\in A} r(s, b) + \gamma f(s')
	& \text{if $s\in\inte{\cG}_n$.}
	\end{cases}
	\end{equation}
	where $s'$ is the next state obtained from $s$ by taking action $b$.
	The updates of both Bellman operators are depicted in \Cref{fig:bellman}.
\end{definition}

\begin{remark}
	For ease of notation, we will sometimes drop the $n$ index on $B$ and $\cB$ when it is non-ambiguous.
\end{remark}


\begin{figure}[tp]
	\centering
	\includegraphics[trim={1.8cm 1.4cm 1.9cm 1.9cm}, clip,width=0.42\linewidth]{img/tree_2}
	\hfill
	\includegraphics[trim={1.8cm 2.6cm 1.9cm 1.3cm}, clip,width=0.46\linewidth]{img/graph_2}
	\caption{Depiction of the Bellman backup operators $B$ (left) and $\cB$ (right). Notice that $B$ only propagates information upward in the tree.}
	\label{fig:bellman}
\end{figure}

\citet{hren2008optimistic} used this Bellman operator $B_n$ in their \texttt{OPD} algorithm to define a pair of bounds $(L_n,\, U_n)$ at each iteration $n$. They use trivial bounds at the leaves, and backup these estimates up to the root by iteratively applying $B_n$. We can show that, under a \textit{consistency} condition (satisfied by the trivial bounds $0$ and $V_{max}$), applying $B_n$ can only tighten a bound and converges in a finite time.

\begin{definition}[Consistency]
	We say that a lower-bound $L$ and an upper bound $U$ are \emph{consistent} if they are respectively non-decreasing and non-increasing along sequences of actions:
	\begin{align*}
	\forall a\in\inte{\Tau}_n, \quad L(a) \leq \max_{b\in A} r(ab) + \gamma L(ab), \quad U(a) \geq \max_{b\in A} r(ab) + \gamma U(ab)
	\end{align*}
\end{definition}

\begin{lemma}[Properties of $B_n$]
	\begin{enumerate}[label=(\roman*)]
		\item $B_n$ preserves consistency;
		\item $B_n$ tightens consistent bounds: $
		L \leq B_n(L) \leq V \leq B_n(U) \leq U
		$;
		\item $(B_n^k)_k$ converges in a finite time $d_n$, where $d_n$ is the depth of $T_n$. 
		
		We denote $B_n^{\infty} \eqdef \lim_{k\rightarrow \infty} B_n^k = B_n^{d_n}$.
	\end{enumerate}
\end{lemma}

This enables \citet{hren2008optimistic} to define non-trivial valid bounds on $V$:
\begin{align}
L_n = B_n^{d_n}(0), \qquad U_n = B_n^{d_n}(V_{\max}).
\end{align}

The corresponding \texttt{OPD} algorithm is described in \Cref{alg:opd}. Likewise, we propose \Cref{alg:gbopd} which follows the same approach adapted to a graph structure. Though the two algorithms share a very similar design, we claim that using graphs provides substantial theoretical and practical performance improvements, and back up this statement in \Cref{sec:analysis,sec:experiments}.

\begin{algorithm}[th]
	\caption{The \emph{Optimistic Planning of Deterministic Systems} (\OPD) algorithm from \citep{hren2008optimistic}.}
	\label{alg:opd}
	\DontPrintSemicolon
	\For{each iteration $n$}{
		Compute the bounds $L_n = B_n^{d_n}(0)$ and $U_n = B_n^{d_n}(V_{\max})$.\; 
		
		$\hat{a}_n$ = $\emptyset$\;
		\While{$\hat{a}_n\in\inte{\Tau}_n$}{
			$\hat{a}_n\gets \displaystyle\argmax_{a'\in \hat{a}_n A} r(a') + \gamma U(a')$ \Comment*[r]{Optimistic sampling rule}
		}
		\For(\Comment*[f]{Node expansion}){action $a\in A$}{
			Simulate $r, s' \sim \probability{r, s' \condbar s(\hat{a}_n), a}$.\;
			Add a new leaf $\hat{a}_n a$ to $\Tau_{n+1}$, with associated reward $r$.
		}
	}
	\Return $\displaystyle\argmax_{a\in A} r(s, a) + \gamma L_n(a)$. \Comment*[r]{Conservative recommendation rule}\;
\end{algorithm}

\begin{algorithm}[th]
	\caption{Our \emph{Graph-Based Optimistic Planning for Deterministic systems} (\GBOPD) algorithm.}
	\label{alg:gbopd}
	\DontPrintSemicolon
	\For{each iteration $n$}{
		Compute the bounds $\cL_n = \cB_n^{\infty}(0)$ and $\cU_n = \cB_n^\infty(V_{\max})$.\; 
		
		$\hat{s}_n$ = $s_0$\;
		\While{$\hat{s}_n\in\inte{\cG}_n$}{
			$\hat{s}_n\gets \displaystyle\argmax_{s'} r(s, a) + \gamma \cU(s')$ \Comment*[r]{Optimistic sampling rule}
		}
		\For(\Comment*[f]{Node expansion}){action $a\in A$}{
			Simulate $r, s' \sim \probability{r, s' \condbar s, a}$.\;
			Get or create the node $s'$ in $\cG_{n+1}$, and add the transition $(s,a) \rightarrow s', r$.
		}
	}
	\Return $\displaystyle\argmax_{a\in A} r(s,a) + \gamma \cL(s(a))$. \Comment*[r]{Conservative recommendation rule}
\end{algorithm}

\section{Analysis}
\label{sec:analysis}

Comparing Algorithms \ref{alg:opd} and \ref{alg:gbopd} directly is difficult since they do not involve the same structure, which implies implicit differences in their behaviours. Considering them under a common framework would make these differences explicit. In order to leverage the previous analyses of the \texttt{OPD} algorithm, we frame our \Cref{alg:gbopd} as a tree-based planning algorithm. To account for the distinction between the operators $B$ and $\cB$, we represent $\cB$ as tree backup $B$ augmented with \emph{merge} operator $M$ which enforces that nodes with similar states share the same value.

\subsection{Background on the sample complexity of \OPD}

First, we recall the analysis of \OPD. We start by introducing some notations.

\begin{definition}[Sequence values]
The value of a finite \textbf{sequence} of actions $a\in A^h$ is:
\begin{equation*}
\label{eq:state_value}
    V(a) = R(s_0,a) + \gamma^{h} V(s(a)),
\end{equation*}
where $R(s, a) = \sum_{t=0}^{h-1} \gamma^t r(a_{1:t})$ is the return of the sequence $a$ starting from the state $s$.
\end{definition}

This enables to define a measure of the difficulty of a planning problem.

\begin{definition}[Difficulty measure]
We define the near-optimal branching factor $\hlrb{\kappa}$ of an MDP as
\begin{equation}
\hlrb{\kappa = \limsup_{h\rightarrow\infty} |\hlrb{\Tau_h^\infty}|^{1/h}} \in [1, K]
\end{equation}
where ${\Tau^\infty}$ is the subtree of near-optimal nodes $${\Tau_h^\infty} = \left\{a\in A^h: V^\star-V(a) \leq \frac{\gamma^h}{1-\gamma}\right\}.$$
\end{definition}

This problem-dependent measure $\hlrb{\kappa}$ is the branching factor of the subtree $T^\infty$ of near-optimal nodes that can be sampled by \OPD, and acts as an effective branching factor as opposed to the true branching factor $K$. When $\kappa$ is small, fewer nodes need to be explored at a given depth, which means that the planner will be able to plan deeper for a given budget $n$. Thus, it directly impacts the simple regret that can be achieved by \OPD when run on a given MDP.


\begin{theorem}[Regret bound of \citealt{hren2008optimistic}]
\label{thm:regret-opd}
The \Cref{alg:opd} enjoys the following regret bound:
\begin{align*}
\quad r_n = \tilde{\cO}\left( n^{-\log \frac{1}{\gamma}/\hlrb{\log\kappa}}\right).
\end{align*}
where $f_n = \tilde{\cO}(n^{-\alpha})$ means that for any $\alpha'<\alpha$, $f_n = \cO(n^{-\alpha'}).$
%where $f_n = \tilde{\cO}(g_n)$ means that $f_n$ is a $\cO(g_n)$ up to an \emph{infinitesimal} polynomial factor: for any $\alpha>0$, $f_n = {\cO}(g_n n^{\alpha}).$
\end{theorem}


$\hlrb{\kappa}$ is typically small in problems where there is one clearly identified optimal trajectory, of which any deviation can be quickly dismissed as suboptimal. Conversely, $\kappa$ is large when there are many suboptimal trajectories that cannot be distinguished easily based on their values, which requires the uniform exploration of the entire tree $T$ with its original branching factor $K$.

%\oam{Recall some examples from literature.}
\oam{Add discussion regarding literature scarcity on the topic}

\subsection{Motivation for an improved regret bound}

We start by reformulating the sampling rule used for the \texttt{OPD} algorithm. To that end, notice that when some bounds $(L,\,U)$ on the state values $V(s(a))$ are available, they also induce bounds $(\overline{L},\, \overline{U})$ on values $V(a)$ of sequences of actions $a\in A^h$ defined as:
\begin{equation*}
%\label{eq:sequence_value}
\underbrace{R(a) + \gamma^{h} L(a)}_{\overline{L}(a)} \leq V(a) \leq \underbrace{R(a) + \gamma^{h} U(a)}_{\overline{U}(a)}
\end{equation*}


One can easily see that, since the $(L_n,\,U_n)$ used in the optimistic sampling rule described in \Cref{alg:opd} are invariant by $B_n$ by definition, this rule can be equivalently expressed as:
\begin{equation}
\label{eq:sampling_rule}
\hat{a}_n \in \argmax_{a\in\ext{\Tau}_n} \overline{U}_n(a).
\end{equation}
Likewise, the conservative recommendation rule returns the first action of:
\begin{equation}
\label{eq:recommendation_rule}
a_n \in \argmax_{a\in\ext{\Tau}_n} \overline{L}_n(a)
\end{equation}


As shown in \Cref{fig:bellman}, in a tree the Bellman operator $B_n$ only propagates the information upward, and the leaves cannot be updated. Thus, $U_n = B_n^{d_n}(V_{\max})$ and $V_{\max}$ coincide on $\ext{\Tau}_n$ which means that the sampling rule of \texttt{OPD} can be summarized as using \eqref{eq:sampling_rule} with the trivial upper-bound $U_n = V_{\max}$.
Likewise, the recommendation rule simply uses \eqref{eq:recommendation_rule} with the trivial lower-bound $L_n = 0$. Thus, \texttt{OPD} amounts to simply using the trivial bound $(0,\, V_{\max})$ on leaf nodes, and does not make use of all the available information in $\Tau_n$ to improve these bounds.

Assume that we instead had access to some tighter bounds $(L,\,U)$ provided by an oracle: $$0\leq L\leq V\leq U\leq V_{\max}.$$
\begin{definition}[A finer difficulty measure]
We define the near-optimal branching factor \emph{according to the bounds $(L,\,U)$} as 
\begin{equation}
\hlbb{\kappa(L,U) \eqdef \limsup_{h\rightarrow\infty} \left|\Tau_h^\infty(L,U)\right|^{1/h}}\in(1, K], 
\end{equation}
where
\begin{equation*}
     {\Tau_h^\infty(L,U)}=\left\{a\in A^h: V^\star - V(a)\leq \gamma^{h}(U(a)-L(a))\right\}.
\end{equation*}
\end{definition}

\begin{lemma}This branching factor shrinks as the bounds $(L,\,U)$ get tighter:
\[L_2\leq L_1\leq V\leq U_1\leq U_2\implies \kappa(L_1,U_1) \leq \kappa(L_2,U_2).\]
In particular, $\hlbb{\kappa(L,U)} \leq \hlrb{\kappa}$.
\end{lemma}

\begin{theorem}
\label{thm:regret-bound-U}
Let $L \leq V\leq U$ consistent bounds, then planning with $L$ and $U$ in \eqref{eq:sampling_rule} and \eqref{eq:recommendation_rule} yields the following simple regret bound:
\begin{equation*}
r_n = \tilde{\cO}\left(n^{-\log \frac{1}{\gamma}/\hlbb{\log \kappa(L,U)}}\right).
\end{equation*}
\end{theorem}


This theorem states that we can potentially improve the performance of the planning algorithm if we manage to find bounds $(L,\, U)$ that are tighter than the trivial ones at the leaves $\ext{\Tau_n}$, but it does not explain how to obtain such bounds. In the next subsection, we describe a method to build at each planning iteration $n$ a sequence of increasingly tight bounds $(L^k,\, U^k)$. The corresponding regret bound, our main result, is stated in \Cref{thm:regret-state-aware}.


\subsection{Merging the states}

In order to reproduce the behaviour of \Cref{alg:gbopd} on a tree structure, we rely on two observations: \begin{enumerate*}[label=(\roman*)]
	\item in a graph, all paths leading to a single state have the same value;
	\item expanding a node in the graph simultaneously expands all the paths that lead to this node.
\end{enumerate*}
They are implemented on top of \Cref{alg:opd} through the definition of two additional operators, shown in \Cref{fig:operators}.


\begin{definition}[Merge operator $M$]
If several sequences $a'\in\Tau_n$ all lead to the same state $s$, their respective bounds must hold simultaneously. We introduce the merge operator $M$ as:
    \begin{align}
    \label{eq:merge}
        \forall a\in\mathcal{T}_n, \qquad &M_n^-(L)(a) = \max_{a'\in \cN_n(a)} L(a'),\qquad M_n^+(U)(a) = \min_{a'\in \cN_n(a)} U(a').                                   
    \end{align}
\end{definition}

\begin{remark} For ease of notation, we sometimes refer to $M_n^-$ and $M_n^+$ as a single merge operator $M_n$, that acts as $M_n^-$ on lower-bounds and $M_n^+$ on upper-bounds.
\end{remark}


\begin{figure}[th]
	\centering
	\includegraphics[trim={1.8cm 1.4cm 1.1cm 1.1cm}, clip,width=0.42\linewidth]{img/tree_3}
	\qquad
	\includegraphics[trim={1.8cm 1.4cm 1.1cm 1.1cm}, clip, width=0.42\linewidth]{img/tree_4}
	\caption{\textbf{Left:} Depiction of the merge operator $M_n$. Nodes can be updated anywhere in the tree $\Tau_n$, and in particular its leaves $\ext{\Tau}_n$. \textbf{Right:} Depiction of the leaves pruning operator $P$. A leaf verifying the criterion of \Cref{prop:pruning} is removed from the set of leaves candidate to expansion.}
	\label{fig:operators}
\end{figure}

\begin{proposition}[Pruning operator $P$]
\label{prop:pruning}
Let $a_1,a_2\in\Tau_n$ such that state $s(a_1) = s(a_2) = s$ and $U = M(U) \geq V$ an aggregated upper-bound. 
\begin{equation}
\label{eq:pruning}
    \text{If } h(a_2) \geq h(a_1) \text{ and } \overline{U}(a_2) \geq \overline{U}(a_1)
    \text{, then }V(a_2) \geq V(a_1)
\end{equation}

In particular, there is no need to ever expand the node $a_1$. We propose that at each step, we detect such pairs $a_1, a_2$. Whenever $a_1$ is a leaf, we can remove it from the set $\ext{\Tau}_n^-\subset \ext{\Tau}_n $ of candidates for expansion.
\end{proposition}

\begin{lemma}[Properties of $M_n$]
	\begin{enumerate*}[label=(\roman*)]
		\item $M_n^2=M_n$;
		\item $M_n$ preserves consistency;
		\item $M_n$ tightens bounds:
		$
		L\leq V \leq U \implies L\leq M^-(L) \leq V \leq M^+(U) \leq U.
		$
	\end{enumerate*}
\end{lemma}

%At this stage, we have defined two operators $M$ and $B$ that operate on bounds $f$ and can only tighten them. As observed in \eqref{eq:sampling_rule}, the sampling rule only depends on the value of $U$ at the leaves $\ext{\Tau}_n$, which makes applying $B$ alone useless since information only travels upwards in the tree $\Tau_n$. Conversely, applying $M$ can update the value of any node in the tree $\Tau_n$, including the leaves $\ext{\Tau}_n$. These tightened bounds at the leaves can then be propagated upward with Bellman backups $B$ to update inner nodes, which can in turn be aggregated with other leaf nodes, and so forth. 
Thus, the graph Bellman backups $\cB^\infty$ correspond to an alternating procedure of backups $B$ and  merges $M$ that operate on bounds $f$ and can only tighten them. The convergence of this procedure must be studied.

\begin{proposition}[Contractivity of $M_nB_n$]
	\label{prop:contractivity}
	$M_n B_n$ a $\gamma$-contraction.
\end{proposition}

We can perform a fixed-point iteration of alternating backups and merges starting from the trivial bounds $(0,\, V_{\max})$.

\begin{equation}
\label{eq:recursion}
L_n^k = (M_n^- B_n)^k(0), \qquad
U_n^k = (M_n^+ B_n)^k(V_{\max})
\end{equation}
$(U_n^k)$ (resp. $(L_n^k)$) is non-increasing (resp. non-decreasing) and converges at a rate $\gamma^k$, but can converge in infinite time whenever there is a loop, as shown in \Cref{sec:convergence}. We can decide to stop whenever a desired accuracy is reached: 

\begin{proposition}[Early stopping]
	\label{prop:early-stopping}
	Provided that
	\[\forall a\in\Tau_n,\; |U^{k+1}(a) - U^k(a)| \leq \epsilon (1-\gamma)\gamma^{-h(a)-1},\]
	the sequence upper-bound $\overline{U^{\infty}}$ is approximated by $\overline{U^{k+1}}$ with an accuracy of $\epsilon$.
\end{proposition}

Finally, at iteration $n$ we can compute the bounds 
\begin{equation}
\label{eq:recursion2}
L_n = (M_n^- B_n)^\infty(0), \qquad
U_n = (M_n^+ B_n)^\infty(V_{\max}),
\end{equation}

used in the tree version of \Cref{alg:gbopd}, that we describe in \Cref{alg:gbop-t}.\\
\begin{algorithm}[H]
	\caption{Tree version of \Cref{alg:gbopd}}
	\label{alg:gbop-t}
	\DontPrintSemicolon
	\For{iteration $n$}{
		Compute $L_n = (M_n^-B_n)^\infty(0)$, $U_n = (M_n^+B_n)^\infty(V_{\max})$ with \eqref{eq:bellman-tree}, \eqref{eq:merge}.\;
		Select the optimistic sequence of actions $\hat{a}_{n}$ from \eqref{eq:sampling_rule}.\;
		\For{action $a\in A$}{
			Add the child $\hat{a}_{n}a$ to the tree $\Tau_{n+1}$ and observe its reward.
		}
		Prune the tree: $\ext{\Tau}_{n+1}^- = P(\ext{\Tau}_{n+1})$.\;
	}
	\Return the recommendation $a_{n+1}$ from \eqref{eq:recommendation_rule}.\;
\end{algorithm}

\subsection{Main result}

In \Cref{thm:regret-bound-U}, we assumed that some bounds $L,\,U$ were revealed by an oracle and available from the onset for planning. In \Cref{alg:gbop-t}, we instead built a \emph{sequence} of bounds $(L_n,U_n)_{n\geq 0}$ \eqref{eq:recursion2} that is non-increasing in the sense of inclusion, i.e. $0\leq \dots\leq L_{n-1}\leq L_n\leq V\leq U_n\leq U_{n-1}\leq \dots\leq V_{\max}$.

We can consider the sequence $\kappa_n = \kappa(L_n, U_n)$. It is non-increasing and lower-bounded by $1$, thus converges. Let $\hlgb{\kappa_\infty = \lim_{n\rightarrow\infty} \kappa(L_n, U_n)} \in[1,K]$.

\begin{theorem}
\label{thm:regret-state-aware}
The \Cref{alg:gbop-t} enjoys the following regret bound with $\hlgb{\kappa_\infty} \leq \hlrb{\kappa}$: 
\begin{align*}
r_n = \tilde{\cO}\left(n^{-\log \frac{1}{\gamma}/\hlgb{\log \kappa_\infty}}\right).
\end{align*}
\end{theorem}

Intuitively, $\kappa_\infty$ will be much lower than $\kappa$ in problems where trajectories overlap a lot. For instance, it will be the case for problems where two actions cancel each-other out (e.g. moving left or right), or are commutative (e.g. placing pawns on a board game).

However, this is merely an intuition. So far we have not proven the existence of problem instances such that $\hlgb{\kappa_\infty} < \hlrb{\kappa}$, which is a legitimate concern since their non-existence would make our \Cref{thm:regret-state-aware} both trivial and uninformative.

\subsection{Illustrative example}
\label{sec:illustrative-example}
We consider a toy MDP $\cM$ shown in \Cref{fig:mdp}. The transitions are described visually while the rewards are defined as follows: let $0\leq r^\star\leq \gamma$, and $ r^- = r^\star - \frac{\gamma}{1-\gamma} S$, $r^+ = r^\star + S$ with $S = r^\star\left(\frac{1}{\gamma} - 1\right).$ Note that the choice ensures that $r^\star, r^-, r^+$ and $S$ are all in $[0, 1]$.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.35\textwidth]{img/mdp.pdf}
    \caption{A toy MDP with three states and $K \geq 2$ actions. We start in the top state. The first action $a_1$ is represented by \textcolor{OliveGreen}{green} arrows, and all other actions $a_2, \dots, a_K$ are represented by \textcolor{red}{red} arrows. The rewards are shown next to the transitions.}
    \label{fig:mdp}
\end{figure}

\begin{proposition}[Comparison of branching factors]
\label{prop:illustrative-example}
The MDP $\cM$ verifies:
\begin{enumerate}[label=(\roman*)]
    \item $\hlrb{\kappa = K-1}$;
    \item $\hlgb{\kappa_\infty = 1}$.
\end{enumerate}
\end{proposition}

Thus, this result shows that our \Cref{thm:regret-state-aware} is non-trivial since we exhibit a problem where $\hlgb{\kappa_\infty} < \hlrb{\kappa}$ (when $K\geq 3$), and legitimates our attempt to improve planning performances by merging the tree into a graph.

\oam{Explain what is natural/expected in this result.  What is perhaps surprising/novel/non-trivial? What impact on practitioners? }


\section{Extension to Stochastic Systems}

Whenever a planning algorithm is based on bounds over the values, our approach can be applied to tighten these bounds while preserving their validity. In particular, when the rewards and/or transitions are stochastic, many algorithms rely on confidence intervals to build statistical bounds on the values that hold with high probability.

\subsection{Episodic planning}


\subsection{Confidence bounds}

\paragraph{Stochastic rewards}

KL-OLOP

\paragraph{Stochastic dynamics}

MDP-GapE ?

Other closed loop algorithms are not runnable, except BRUE which is not optimistic and does not involve bounds.

By tightening the bounds, we preserve the guarantees that rely on their validity and rate of convergence. 

\section{Numerical Experiments}
\label{sec:experiments}

\oam{Better explain the figures shown in this section}

\oam{On top of the illustrative pictures, would it be possible to compute some simple score, say 
for each $k\in\mathbb{N}$,  count the number $o_k$ of occupancies at distance $k$ from the goal, then look at a weighted sum, $\sum_{k} w_k o_k$,
where $w_k$ is decreasing ($1/k$, or $\alpha^k$, etc.) ?
}

% \paragraph{Illustrative example}
% We consider the example problem of \Cref{sec:illustrative-example} with $r_1=0$ and $r_2 = 0.7$. As expected from \Cref{prop:illustrative-example}, the tree expanded by \texttt{OPD} is quite balanced, even with such a dense reward and important gap. In contrast, The state-aware planner never expands a suboptimal node.
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.49\textwidth]{img/bandit_deterministic.pdf}
%     \includegraphics[width=0.49\textwidth]{img/bandit_state_aware.pdf}
%     \caption{Trees expanded for $n = 200$, $\gamma=0.99$}
%     \label{fig:bandit_trees}
% \end{figure}

\paragraph{Gridworld}

The reward is a paraboloid centred at $(10, 10)$ with length-scale of 5:  $r(x, y) = 1 - \frac{1}{5^2}((x-10)^2 + (y-10)^2)$ clipped to [0, 1].  for $n = 5460$, $\gamma=0.95$

\begin{figure}[t]
	\centering
	\includegraphics[trim={1.8cm 0.7cm 1.8cm 0.7cm}, clip, width=0.49\textwidth]{img/occupations_OPD.pdf}
	\hfill
	\includegraphics[trim={1.8cm 0.7cm 1.8cm 0.7cm}, clip, width=0.49\textwidth]{img/occupations_GBOP-D.pdf}
	\caption{Deterministic Gridworld}
	\label{fig:visits-d}
\end{figure}
\begin{figure}[t]
	\centering
	\includegraphics[trim={1.8cm 0.7cm 1.8cm 0.7cm}, clip, width=0.49\textwidth]{img/occupations_MDP-GapE.pdf}
	\hfill
	\includegraphics[trim={1.8cm 0.7cm 1.8cm 0.7cm}, clip, width=0.49\textwidth]{img/occupations_GBOP.pdf}
	\caption{Stochastic Gridworld}
	\label{fig:visits-s}
\end{figure}


\paragraph{Sailing problem}

\section*{Conclusion}


\bibliography{references}


\clearpage
\appendix

\section{Proofs}
\subsection{Proof of \Cref{thm:regret-opd}}
We simply recall the sketch of the proof of \citet{hren2008optimistic}.\\

\begin{enumerate}
	\item The recommendation $a_n$ has a maximal depth $d_n$ in the tree, and its gap $r_n = V^\star - V({a_{n,1}})$ is bounded by $r_n \leq \frac{\gamma^{d_n}}{1-\gamma}$. We need to relate $d_n$ to $n$.
	
	\item Each expanded node belongs to $\Tau^\infty = \bigcup_{h\geq 0} \Tau_h^\infty$, where $$\Tau_h^\infty = \left\{a\in A^h: V^\star-V(a) \leq \frac{\gamma^h}{1-\gamma}\right\}.$$ Introduce the difficulty measure $\kappa$ such that $|\Tau_h^\infty| = \cO(\kappa^h)$ (the smallest).
	
	\item In the worst case, expanded nodes fully fill the depths of $\Tau^\infty$ up to $d_n$: $n = \sum_{d=1}^{d_n} n_d \leq  C\sum_{d=1}^{d_n} \kappa^d = \begin{cases}
	\cO(d_n) &\text{if $\kappa=1$}\\
	\cO(\kappa^{d_n}) &\text{else.}
	\end{cases}$\\
	Hence $r_n = \begin{cases}
	\cO(\gamma^n) &\text{if $\kappa=1$}\\
	\cO(\gamma^{\frac{\log n}{\log \kappa}}) = \cO(n^{-\frac{\log 1/\gamma}{\log \kappa}}) &\text{else.}
	\end{cases}$
\end{enumerate}

\subsection{Proof of \Cref{thm:regret-bound-U}}
In this proof, we temporarily assume that $U=B(U)$ and $L=B(L)$. We follow the same steps as in the proof of the regret of \texttt{OPD}.

\begin{remark}
It does not hold anymore that $a_n$ must be of maximal depth $d_n$.  This is due to the fact the exploration bonus $\gamma^h U(a)$ is not depth-wise constant: consider two nodes $a,b$ at the same depth with $R(a) > R(b)$. In \texttt{OPD}, both get the same bonus $\gamma^h/(1-\gamma)$, and the node $a$ is expanded first. But with the local bonus, $b$ could be expanded in priority rather than $a$, if its own bonus is sufficiently higher than that of $a$, precisely if $R(a)+\gamma^h U(a) < R(b)+\gamma^h U(b)$. For instance, $U(a)=0$ when $a$ is known to be a terminal state while $b$ can lead to future rewards. If after expanding and exploring the subtree of $b$ we find out that $V(b) = 0$, we still return the recommendation $a$, which is of non-maximal depth.
\end{remark}

The regret bound still holds, however. First, notice that:
\begin{lemma}
\label{lemma:expansion-bound}
Whenever a node $a$ of depth $h$ is expanded by the optimistic algorithm, its first action $a_1$ enjoys a simple regret $V(a^\star)-V(a_1) \leq \gamma^h(U(a)-L(a))$. 
\end{lemma}
\begin{proof}
Let $t$ be the time of expansion of $a$, it holds that $\overline{U}_t(b) \leq \overline{U}_t(a)$ for all $b\in \ext{\Tau}_t$, in particular those in a branch starting by an optimal action $a^\star$. Since $U=B(U)$ and $L=B(L)$, we also have $\overline{U}_t(a^\star) = \max_{b\in a^\star A^*} \overline{U}_t(b) \leq \overline{U}_t(a)$, and $\overline{L}_t(a_1) = \max{b\in a_1 A^*} \overline{L}_t(b) \geq  \overline{L}_t(a)$. Thus, $V(a^\star)-V(a_1) \leq \overline{U}_t(a^\star) - \overline{L}_t(a_1) \leq \overline{U}_t(a) - \overline{L}_t(a) = \gamma^h(U(a)-L(a))$.
\qed\end{proof}
 
\begin{lemma}
The recommended action $a_n$ has a simple regret $r_n \leq \frac{\gamma^{d_n}}{1-\gamma}$, where $d_n$ is the maximal depth of $\Tau_n$.
\end{lemma}
\begin{proof}
Let $i$ a node of maximal depth $d_n$, and consider the recommended node $a_n$ at time $n$, of depth $d$. In particular, $\overline{L}_n(a_n) \geq \overline{L}_n(i)$, and since $(\overline{L}_t)_t$ is non-decreasing we also have $\overline{L}_n(i) \geq \overline{L}_t(i)$. At the time $t$ when $i$ is expanded, we have $\overline{U}_t(a_n) \leq \overline{U}_t(i)$, and since $(\overline{U}_t)_t$ is non-increasing we also have $\overline{U}_n(a_n) \leq \overline{U}_t(a_n)$. We can conclude with \Cref{lemma:expansion-bound} applied to $a_n$: $r_n \leq \gamma^d(U(a_n)-L(a_n) = \overline{U}_n(a_n) - \overline{L}_n(a_n)  \leq \overline{U}_t(a_n) - \overline{L}_n(i) \leq \overline{U}_t(i) - \overline{L}_t(i) = \gamma^{d_n}(U(i) - L(i)$, which yields the claimed bound since $U(i) - L(i) \leq V_{\max}-0$.
\qed\end{proof}

\begin{lemma}
\label{lemma:expansion-tree}
Every node expanded by \eqref{eq:sampling_rule} is in $\Tau^\infty(L,U) = \bigcup_{h\geq 0} \Tau^\infty_h(L,U)$.
\end{lemma}
\begin{proof}
Let $a$ be a node of depth $h$ expanded at round $n$, then $\overline{U}_n(a) \geq \overline{U}_n(b)$ for all $b\in\ext{\Tau}_n$. Thus, since $U = B(U)$, we have $\overline{U}(a) = \overline{B(U)}(\emptyset) = B(U)(s_0) \geq V(s_0) = V^\star$. Thus, $V^\star - V(a) \leq \overline{U}(a) - \overline{L}(a) = \gamma^h(U(a) - L(a))$.
\qed\end{proof}

Finally, we can move on to the proof of \Cref{thm:regret-bound-U}.
Let $n_d$ be the number of expanded nodes of depth $d$, by \Cref{lemma:expansion-tree} we have $n_d \leq |\Tau^\infty_d(L,U)| \leq C\kappa(L,U)^d$. Thus, 
\[n = \sum_{d=1}^{d_n} n_d \leq C\sum_{d=0}^{d_n} \kappa(L,U)^d = C\frac{\kappa(L,U)^{d_n+1}-1}{\kappa(L,U)-1}\]
Hence, $d_n \geq C'\frac{\log n}{\log\kappa(L,U)},$ which along with \Cref{lemma:expansion-bound} gives the claimed bound.

Note that if $L,\,U$ are consistent bounds that do not verify $L = B(L)$ and $U=B(U)$, then planning with $B(L),B(U)$ instead will yield the proved bound with a branching factor $\kappa(B(L),B(U))$, and since $L\leq B(L)\leq V\leq B(U)\leq U$ we have $\kappa(B(L),B(U)) \leq \kappa(L,U)$, which still gives \begin{align*}
r_n = \cO\left(n^{-\frac{\log 1/\gamma}{\log \kappa(L,U)}}\right);
\end{align*}

\subsection{Proof of \Cref{prop:contractivity}}
\begin{proof}
Let $U_1, U_2\in \Real^\Tau_n, a\in\Tau_n$,
\begin{align*}
    (M U_1 - M U_2)(a) &= \min_{a'\in\cN_n(a)} U_1(a') - \min_{a'\in\cN_n(a)} U_2(a') \\
    &= \min_{a'\in\cN_n(a)} U_1(a') - U_2(a^-) \\
    &\leq U_1(a^-) - U_2(a^-) \\
    &\leq \|U_1 - U_2\|_\infty
\end{align*}
where $a^-\in \argmin_{a'\in\cN_n(a)} U_2(a')$. 
Hence, $\|M U_1 - M U_2\|_\infty \leq \|U_1 - U_2\|_\infty$
\qed\end{proof}

\subsection{Proof of \Cref{prop:early-stopping}}

\begin{proof}
Let $a\in A^h$. We consider the sequence $(\overline{U}_n)_{n\in\Natural}$.
Notice that for any $U,V\in\Real^\Tau$, we have $\overline{U}(a)-\overline{V}(a)=\gamma^h(U(a)-V(a))$.

Hence, if the premise holds,
\begin{align*}
    |\overline{U}_{k+1}(a) - \overline{U}_{k}(a)| &\leq \gamma^h\epsilon (1-\gamma)\gamma^{-h-1} = \epsilon (1-\gamma)\gamma^{-1}
\end{align*}

And then,
\begin{align*}
|\overline{U}_{k+1}(a) - \overline{U}_\infty(a)| &= \gamma^h |U_{k+1}(a) - U_\infty(a)|\\
&\leq \gamma^{h+1}|U_{k}(a) - U_\infty(a)| \text{ since $LA$ is a $\gamma$-contraction}\\
&\leq \gamma^{h+1}|U_{k}(a) - U_{k+1}(a)| + \gamma^{h+1}|U_{k+1}(a) - U_\infty(a)|\\
&= \gamma|\overline{U}_{k}(a) - \overline{U}_{k+1}(a)| + \gamma |\overline{U}_{k+1}(a) - \overline{U}_\infty(a)|\\
&\leq \frac{\gamma}{1-\gamma} |\overline{U}_{k}(a) - \overline{U}_{k+1}(a)|\\
&\leq\epsilon
\end{align*}
\qed\end{proof}

\subsection{Proof of \Cref{prop:pruning}}

\begin{proof}
Assume $h(a_2) \geq h(a_1)$.
\begin{align*}
    V(a_1) - V(a_2) &= R(a_1)- R(a_2) + \underbrace{\left(\gamma^{h(a_1)} - \gamma^{h(a_2)}\right)}_{\geq 0}V(s) \\
    &\leq R(a_1)- R(a_2) + \left(\gamma^{h(a_1)} - \gamma^{h(a_2)}\right)U(s)\\
    &= \overline{U}(a_1) - \overline{U}(a_2)
\end{align*}
Hence, if this last term is negative, then $V(a_1) - V(a_2)$ is as well.
\qed\end{proof}

\subsection{Proof of \Cref{thm:regret-state-aware}}

\begin{proof}
Let $\kappa'>\kappa_\infty$. Since $\kappa(L_n,U_n)\rightarrow\kappa_\infty$, there exists $n_0\in\Natural$ such that for all $n\geq n_0$, $\kappa(L_n,U_n) \leq \kappa'$.
We can show that at each iteration $n$, the expanded node must belong to $\Tau^\infty(L_n,U_n)$.
Let $n\geq n_0$, and define $d_0 = \min\{d\in\Natural: \exists t \in[n_0,n], \hat{a}_t\in A^d \}$. By definition, for all $d\geq d_0$, any expanded node of depth $d$ was expanded at a time $t\geq n_0$, and thus $\hat{a}_t\in\Tau^\infty_t \subset\Tau^\infty_{n_0}$. By denoting $n_d$ the number of expanded nodes of depth $d$, we obtain:
\[
n = \sum_{d=0}^{d_0-1}n_d + \sum_{d=d_0}^{d_n} n_d \leq  C_0 + C_1\sum_{d=d_0}^{d_n} (\kappa')^d \leq C_0 + C_1' (\kappa')^{d_n}
\]
And since $r_n \leq \frac{\gamma^{d_n}}{1-\gamma}$, we obtain the claimed bound.
\qed\end{proof}

\subsection{Proof of \Cref{prop:illustrative-example}}

The \Cref{fig:mdp-tree} shows the planning tree corresponding to the MDP $\cM$. Whenever the action $a_1$ is taken \textcolor{OliveGreen}{(in green)} the resulting subtree is represented by a leaf node $s^\star$ of value $V^\star = \frac{r^\star}{1-\gamma}$. When, in contrast, we take a sequence of actions among $a_2\dots a_K$ \textcolor{red}{(in red)}, we stay in the state $s^+$ and denote $V_h$ the corresponding value at depth $h$.

\begin{figure}
    \centering
    \includegraphics[trim={3.5cm 2cm 0.5cm 0.5cm}, clip, width=0.6\textwidth]{img/mdp_tree.pdf}
    \caption{Planning tree of the MDP $\cM$ of \Cref{fig:mdp}}.
    \label{fig:mdp-tree}
\end{figure}
\begin{lemma}
 Any sequence of actions in $M\setminus{a_1}$ is in $\Tau^\infty$.
\end{lemma}
\begin{proof}
Any such sequence of actions yields the sequence of rewards $r^-, r^+, \dots,r^+$. and end up in the state $s^+$ with value at least $V^\star$ (obtained by further taking $a_1$ indefinitely). Thus its value $V_h$ verifies, 
\begin{align*}
    V_h &\geq \sum_{t=0}^{h-1} \gamma^t r_t + \gamma^h V^\star\\
    &= r^- - r^+ + \sum_{t=0}^{h-1} \gamma^t r^+ + \gamma^h V^\star \\
    &= (-\frac{\gamma}{1-\gamma} - 1)S + \frac{1-\gamma^h}{1-\gamma} (r^\star + S) + \gamma^h V^\star\\
    &= (-\frac{\gamma}{1-\gamma} - 1)S + \frac{1-\gamma^h}{1-\gamma} (r^\star + S) + \gamma^h V^\star\\
    &= V^\star - S\frac{\gamma^h}{1-\gamma} \geq V^\star - \frac{\gamma^h}{1-\gamma}
\end{align*}
\qed\end{proof}

We can directly conclude that $\kappa \geq \limsup{|\{a_2,\dots,a_K\}^h|^{1/h}} = K-1$.

Now, consider the nodes expanded by \Cref{alg:gbop-t}. The first expansion is that of the root, which discovers $s^\star$ and $s^+$. In the absence of information on these two state, the bound $V_{\max}$ is used and the first action $a_1$ gets a higher $\overline{U}$ that any other action $a_2,\dots,a_K$ since $r^\star \geq r^-$. Hence, at the second iteration, the node $a_1$ gets expanded. At this point, the self-loop of the state $s^\star$ is discovered, which means that form now on the bounds verify $L_n(a_1) = V^\star = U_n(a_1)$ for $n\geq2$, which means that $L_n(a_1A^*)-U_n(a_1A^*) = 0$. The nodes $a_2,\dots,a_K$ can be expanded at most once until the entire MDP is discovered and $L_n=V=U_n$ over the entire tree, which means that $\Tau_n^\infty$ is the set of optimal nodes, that is the nodes in the only optimal sequence $a_1^\star$. Hence, $\kappa_\infty = 1.$ 

\section{Convergence of $(M B)^k$}
\label{sec:convergence}
\begin{figure}[H]
    \centering
    \includegraphics[trim=2.5cm 1cm 1.5cm 2cm, clip, width=0.4\textwidth]{img/loop.pdf}\\
    \begin{tabular}{cccccc}
         \toprule
         Operator & $I$ & $B$ & $M B$ & $\cdots$ & $(M B)^k$ \\
         \midrule
         $U(a)$ & $V_{\max}$ & $\frac{1}{2} + \gamma V_{\max}$ & $\frac{1}{2} + \gamma V_{\max}$ && $\frac{1}{2}(1-\gamma^k)V_{\max} + \gamma^k V_{\max}$\\
         $U(b)$ & $V_{\max}$ & $V_{\max}$ & $\frac{1}{2} + \gamma V_{\max}$ && $\frac{1}{2}(1-\gamma^k)V_{\max} + \gamma^k V_{\max}$\\
         $L(a)$ & $0$ & $\frac{1}{2}$ & $\frac{1}{2}$ && $\frac{1}{2}(1-\gamma^k)V_{\max}$\\
		 $L(b)$ & $0$ & $0$ & $\frac{1}{2}$ && $\frac{1}{2}(1-\gamma^k)V_{\max}$\\
         \bottomrule
    \end{tabular}
    \caption{\textbf{Top}: a looping MDP with $|S|=|A|=1$, and the corresponding expanded tree $\Tau_1$ for a single observed transition. \textbf{Bottom}: the sequence of upper-bounds $(U_n)$ when alternating $A$ and $L$. $(U_k)$ and $(L_k)$ converge geometrically to their limit $V = \frac{1}{2}V_{\max}$, thus in infinite time.} % TODO: r=1/2
    \label{fig:simple_loop}
\end{figure}

\section{Supplementary Experiments}

\subsection{Trees expanded}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.44\textwidth]{img/tree_OPD.pdf}
%    \includegraphics[width=0.44\textwidth]{img/tree_KL-OLOP.pdf}
	\includegraphics[width=0.44\textwidth]{img/tree_GBOP-D.pdf}
	\caption{Trees expanded for $n = 5460$, $\gamma=0.95$}
	\label{fig:trees_expanded}
\end{figure}

\subsection{Effect of the early stopping}

\begin{figure}[H]
	\centering
	% \includegraphics[width=0.49\textwidth]{img/4_updates_deterministic.pdf}
	% \includegraphics[width=0.49\textwidth]{img/4_updates_kl-olop.pdf}
	\includegraphics[width=0.44\textwidth]{img/updates_GBOP-D.pdf}
	\caption{Number of updates in the leaf expansion for $n = 5460$, $\gamma=0.95$}
	\label{fig:gw4_updates}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=0.49\linewidth]{img/epsilon/0/updates_state_aware.pdf}
        \includegraphics[width=0.49\linewidth]{img/epsilon/0/occupations_state_aware.pdf}
        \caption{$\epsilon=0$}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=0.49\textwidth]{img/epsilon/1e-2/updates_state_aware.pdf}
        \includegraphics[width=0.49\textwidth]{img/epsilon/1e-2/occupations_state_aware.pdf}
        \caption{$\epsilon=1e-2$}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=0.49\textwidth]{img/epsilon/1e-1/updates_state_aware.pdf}
        \includegraphics[width=0.49\textwidth]{img/epsilon/1e-2/occupations_state_aware.pdf}
        \caption{$\epsilon=1e-1$}
    \end{subfigure}
    \caption{Updates and occupancies for various values of $\epsilon$, for $n = 5460$, $\gamma=0.95$}
    \label{fig:epsilon_1}
\end{figure}
\begin{figure}[H]
\ContinuedFloat
    \centering
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=0.49\textwidth]{img/epsilon/1e0/updates_state_aware.pdf}
        \includegraphics[width=0.49\textwidth]{img/epsilon/1e0/occupations_state_aware.pdf}
        \caption{$\epsilon=1e0$}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=0.49\textwidth]{img/epsilon/1e1/updates_state_aware.pdf}
        \includegraphics[width=0.49\textwidth]{img/epsilon/1e1/occupations_state_aware.pdf}
        \caption{$\epsilon=1e1$}
    \end{subfigure}
    \begin{subfigure}[b]{\textwidth}
        \centering
        \includegraphics[width=0.49\textwidth]{img/epsilon/1e2/updates_state_aware.pdf}
        \includegraphics[width=0.49\textwidth]{img/epsilon/1e2/occupations_state_aware.pdf}
        \caption{$\epsilon=1e2 = V_{\max}$}
    \end{subfigure}
    \caption{Updates and occupancies for various values of $\epsilon$, for $n = 5460$, $\gamma=0.95$}
\end{figure}


\end{document}
