\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{algorithm2e}
\usepackage{caption}
\usepackage{subcaption}

\input{mathdef.tex}

\title{Planning with States}
\author{Edouard Leurent}
\date{October 2019}

\begin{document}

\maketitle

\tableofcontents

\section{Motivation}

Traditional planning algorithms sample sequences of actions by considering sequences of rewards only. Can we leverage state information during planning, to account for the fact that different sequences can lead to similar states, and hence similar future rewards?

\section{Deterministic Systems}

We consider an MDP with discrete state space and deterministic dynamics. We plan from an initial state $s_0$?
\begin{paragraph}{Notations}
At iteration $n$, we consider the graph $\Gamma_n$ that represent all the states and transitions observed so far. It contains exactly $n-1$ \emph{internal} nodes that correspond to states $s$ that have been expanded, and a set $\cS_n$ of up to $n(K-1)+1$ \emph{external} nodes (or \emph{sink} nodes) that correspond to states $s$ that have been observed but not expanded yet. Thus, these \emph{sink} nodes have no outgoing transition. Each observed transition in the graph is annotated with its reward $r(s,a)$
We denote $s(a)$ the state reached after running a sequence of actions $a$.
\end{paragraph}

\begin{definition}[Values]
The value of a \textbf{state} $s\in S$ is:
\begin{equation}
    V(s) = \max_{a\in A^\infty} \sum_{t=0}^\infty \gamma^t r(s_t, a_t) \quad \text{with $s_0=s$},
\end{equation}



and the value of a \textbf{sequence} of actions $a\in A^h$ is:
\begin{equation}
\label{eq:state_value}
    V(a) = \sum_{t=0}^{h-1}\gamma^t r(s_t, a_t) + \gamma^{h} V(s(a)).
\end{equation}
\end{definition}

\begin{definition}[Upper confidence bounds]

We denote by $U:\Gamma_n \rightarrow \Real$ an upper-bound for the state-value $V(s)$ that verifies:
\begin{equation*}
    \forall s\in\Gamma_n, \qquad V(s) \leq U(s)
\end{equation*}

For instance, since we assume that the rewards are bounded in [0, 1], a trivial upper-bound on $V(s)$ is:
\[V(s) \leq V_{\max} = \frac{1}{1-\gamma} \]

The state-value bound $U$ induces an upper-bound $B(U)$ for sequences values defined as:
\begin{equation}
\label{eq:sequence_value}
    V(a) \leq B(U)(a) \eqdef \sum_{t=0}^{h-1} \gamma^t r(s_t, a_t) + \gamma^{h} U(s(a))
\end{equation}
\end{definition}

This provides an optimistic sampling rule for the next state to expand:
\begin{equation}
    \label{eq:sampling_rule}
    a_{n+1} \in \argmax_{\substack{a\in A^*\\s(a)\in\cS_n}} B(U)(a)
\end{equation}

Our goal is to make this bound $U$ as tight as possible given available information.

\begin{definition}[Bellman Optimality operator]
We define the Bellman Optimality operator $L$ over $\Real^{\Gamma_n}$ as:

\begin{equation}
    L(U)(s) = \begin{cases}
    U(s) & \text{if $s\in\cS_n$;} \\
    \min(\max_{a\in A} r(s,a) + \gamma U({s'}),\, U(s))
    & \text{if $(s,a,s')\in\Gamma_n$.}
    \end{cases}
\end{equation}

Note that a $\min$ was added to the traditional Bellman backup in order to maintain the upper-bound of a node if it is already  tighter than the backup of its children.
\end{definition}

\paragraph{Value Iteration}
We can easily see that $L$ is a $\gamma$-contraction and that
$
    V \leq U \implies V \leq L(U) \leq U.
$
The Value Iteration algorithm applies the Bellman operator $L$ iteratively until convergence, to build the sequence $(U_k)$ such that $U_0 = V_{\max}$ and $U_{k+1} = L(U_k)$. We denote its limit as $U_\infty$, which is the tightest bound we obtain: $V\leq U_\infty\leq\dots\leq U_2 \leq U_1\leq V_{\max}$.
Algorithm \ref{alg:state-aware} describes the corresponding algorithm.


\section{Regret bound}

\paragraph{Near-optimality}

We define the branching factor $\kappa(U)\in[1, K]$ of near-optimal nodes \emph{according to a state-value bound $U$} as:
\begin{equation}
    \kappa(U) \eqdef \limsup_{h\rightarrow\infty} \left|\left\{a\in A^h: V^* - V(a)\leq \gamma^{h}U(s(a))\right\}\right|^{1/h}.
\end{equation}
This branching factor shrinks as the bound $U$ gets tighter:
\begin{lemma}
If $U_1$ is a tighter bound that $U_2$, its corresponding effective branching factor is lower:
\[U_1\leq U_2\implies \kappa(U_1) \leq \kappa(U_2).\]
\end{lemma}


\begin{theorem}
If $U$ is a valid upper-bound on the state-value $V\leq U$, then optimistic planning with $B(U)$ gives the following simple regret bound: %$\forall \kappa>\kappa(U)$,
\begin{align*}
V^* - V({a_n}) = \cO\left(n^{-\frac{\log 1/\gamma}{\log \kappa(U)}}\right);
\end{align*}
\end{theorem}

And in particular, planning with $U=U_\infty$ gives a potentially better bound $\kappa(U_\infty)$ than planning with $U=V_{\max}$ as in \texttt{OPD}.

\begin{proof}
Let us consider $\Tau_n$ (and $\cL_n$), the tree (and its leaves) obtained by unrolling the graph $\Gamma_n$ from $s_0$ (and its sinks $\cS_n$).
\end{proof}


\begin{algorithm}[htp]
  \caption{State-aware planning}
  \label{alg:state-aware}
  \SetAlgoLined\DontPrintSemicolon
  \For{iteration $n$}{
  \nl Select the optimistic sequence of actions $a_{n+1}$ from \eqref{eq:sampling_rule} \;
  \nl Expand the corresponding sink node $s = s(a_{n+1})$\;
  \For{action $a\in A$}{
    Observe the transition $s, a, s', r(s,a)$, and add it to the graph $\Gamma_n$
  }
  }
  \Return $\max_{a\in A} r(s_0,a) + \gamma U(s')$\;
\end{algorithm}
\end{document}
