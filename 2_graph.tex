\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{algorithm2e}
\usepackage{caption}
\usepackage{subcaption}

\input{mathdef.tex}

\usepackage{todonotes}
\newcommand{\oam}[1]{\todo[inline,color=orange!40]{{\textbf{OM:}~}#1}}

\title{Planning with States}
\author{Edouard Leurent}
\date{October 2019}

\begin{document}

\maketitle

\tableofcontents


\section{Deterministic Systems}

We consider a deterministic MDP. We plan from an initial state $s_0$?
\begin{paragraph}{Notations}
At iteration $n$, we consider the graph $\Gamma_n$ that represent all the states and transitions observed so far. It contains exactly $n-1$ \emph{internal} nodes that correspond to states $s$ that have been expanded, and a set $\cS_n$ of up to $n(K-1)+1$ \emph{external} nodes (or \emph{sink} nodes) that correspond to states $s$ that have been observed but not expanded yet. Thus, these \emph{sink} nodes have no outgoing transition. Each observed transition in the graph is annotated with its reward $r(s,a)$
We denote $s(a)$ the state reached after running a sequence of actions $a$.
\end{paragraph}

\begin{definition}[Values]
The value of a \textbf{state} $s\in S$ is:
\begin{equation}
    V(s) = \max_{a\in A^\infty} \sum_{t=0}^\infty \gamma^t r(s_t, a_t) \quad \text{with $s_0=s$},
\end{equation}



and the value of a \textbf{sequence} of actions $a\in A^h$ is:
\begin{equation}
\label{eq:state_value}
    V(a) = \sum_{t=0}^{h-1}\gamma^t r(s_t, a_t) + \gamma^{h} V(s(a)).
\end{equation}
\end{definition}

\begin{definition}[Upper confidence bounds]

We denote by $U:\Gamma_n \rightarrow \Real$ an upper-bound for the state-value $V(s)$ that verifies:
\begin{equation*}
    \forall s\in\Gamma_n, \qquad V(s) \leq U(s)
\end{equation*}

For instance, since we assume that the rewards are bounded in [0, 1], a trivial upper-bound on $V(s)$ is:
\[V(s) \leq V_{\max} = \frac{1}{1-\gamma} \]

The state-value bound $U$ induces an upper-bound $B(U)$ for sequences values defined as:
\begin{equation}
\label{eq:sequence_value}
    V(a) \leq B(U)(a) \eqdef \sum_{t=0}^{h-1} \gamma^t r(s_t, a_t) + \gamma^{h} U(s(a))
\end{equation}
\end{definition}

This provides an optimistic sampling rule for the next state to expand:
\begin{equation}
    \label{eq:sampling_rule}
    a_{n+1} \in \argmax_{\substack{a\in A^*\\s(a)\in\cS_n}} B(U)(a)
\end{equation}

Our goal is to make this bound $U$ as tight as possible given available information.

\begin{definition}[Bellman Optimality operator]
We define the Bellman Optimality operator $L$ over $\Real^{\Gamma_n}$ as:

\begin{equation}
    L(U)(s) = \begin{cases}
    U(s) & \text{if $s\in\cS_n$;} \\
    \min(\max_{a\in A} r(s,a) + \gamma U({s'}),\, U(s))
    & \text{if $(s,a,s')\in\Gamma_n$.}
    \end{cases}
\end{equation}

Note that a $\min$ was added to the traditional Bellman backup in order to maintain the upper-bound of a node if it is already  tighter than the backup of its children.
\end{definition}

\paragraph{Value Iteration}
We can easily see that $L$ is a $\gamma$-contraction and that
$
    V \leq U \implies V \leq L(U) \leq U.
$
The Value Iteration algorithm applies the Bellman operator $L$ iteratively until convergence, to build the sequence $(U_k)$ such that $U_0 = V_{\max}$ and $U_{k+1} = L(U_k)$. We denote its limit as $U_\infty$, which is the tightest bound we obtain: $V\leq U_\infty\leq\dots\leq U_2 \leq U_1\leq V_{\max}$.
Algorithm \ref{alg:state-aware} describes the corresponding algorithm.


\section{Regret bound}

\paragraph{Near-optimality}

We define the branching factor $\kappa(U)\in[1, K]$ of near-optimal nodes \emph{according to a state-value bound $U$} as:
\begin{equation}
    \kappa(U) \eqdef \limsup_{h\rightarrow\infty} \left|\left\{a\in A^h: V^* - V(a)\leq \gamma^{h}U(s(a))\right\}\right|^{1/h}.
\end{equation}
This branching factor shrinks as the bound $U$ gets tighter:
\begin{lemma}
If $U_1$ is a tighter bound that $U_2$, its corresponding effective branching factor is lower:
\[U_1\leq U_2\implies \kappa(U_1) \leq \kappa(U_2).\]
\end{lemma}


\begin{theorem}
If $U$ is a valid upper-bound on the state-value $V\leq U$, then optimistic planning with $B(U)$ gives the following simple regret bound: %$\forall \kappa>\kappa(U)$,
\begin{align*}
V^* - V({a_n}) = \cO\left(n^{-\frac{\log 1/\gamma}{\log \kappa(U)}}\right);
\end{align*}
\end{theorem}

And in particular, planning with $U=U_\infty$ gives a potentially better bound $\kappa(U_\infty)$ than planning with $U=V_{\max}$ as in \texttt{OPD}.

\begin{proof}
Let us consider $\Tau_n$ (and $\cL_n$), the tree (and its leaves) obtained by unrolling the graph $\Gamma_n$ from $s_0$ (and its sinks $\cS_n$).
\end{proof}


\begin{algorithm}[htp]
  \caption{State-aware planning}
  \label{alg:state-aware}
  \SetAlgoLined\DontPrintSemicolon
  \For{iteration $n$}{
  \nl Select the optimistic sequence of actions $a_{n+1}$ from \eqref{eq:sampling_rule} \;
  \nl Expand the corresponding sink node $s = s(a_{n+1})$\;
  \For{action $a\in A$}{
    Observe the transition $s, a, s', r(s,a)$, and add it to the graph $\Gamma_n$
  }
  }
  \Return $\max_{a\in A} r(s_0,a) + \gamma U(s')$\;
\end{algorithm}


\section{Extension to Stochastic MDPs}

If the transitions and rewards are stochastic, we can define 
state-aware upper/lower confidence bounds on the values in the following way. At each planning iteration,
\begin{enumerate}
    \item Make the graph of observed states and transitions $N(s), N(s,a), N(s,a,s'), R(s,a,s')$
    \item Unseen transitions $(s,a)$ go to the anonymous state $s_\emptyset$
    \item Estimate $r(s,a)$ and $p(s,a,s')$ based on previous observed samples
    \item Initialise $U_0 = V_{\max}, L_0 = 0$
    \item Backup each $(s,a)$ with $U_{k+1}(s) = \min(U_k(s), u(s,a) + \overline{p}_{s,a} U_k, U_k)$, until convergence (same resp. for $L$)
    \end{enumerate}
The memory complexity of iteration $n$ corresponds to maintaining a set of $n < |\mathcal{S}|$ state estimates and $nKB$ transitions. The planning algorithm then samples the next sequence based on the obtained intervals: start from the root and follow a BAI / Optimistic policy.

TODO: try on sailing env? or gridworld with noise: e.g. 0.5 chance of uneffective action. Plot the state occupancy: number of times a state was expanded ($N(s) > 1$)
    

\end{document}
